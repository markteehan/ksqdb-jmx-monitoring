# Start Confluent Platform specifying two options: (-d) to run in detached mode and (--build) 
#to build the Kafka Connect image with the source connector kafka-connect-datagen from Confluent Hub.
#You must allocate a minimum of 8 GB of Docker memory resource. The default memory allocation on Docker Desktop for Mac is 2 GB and must be changed.
P_GITHUB_REPO=markteehan

export DT=`date "+%Y%m%d-%H%M%S"`
export DT2=`date "+%H%m%S"`


run()
{
 clear
 echo;echo $1
}


runSQL()
{
  echo "set 'auto.offset.reset'='earliest'; " > /tmp/cmda.sql
  cat /tmp/cmda.sql /tmp/cmd.sql > /tmp/cmdb.sql
  docker cp /tmp/cmdb.sql ksqldb-cli:/tmp/cmd.sql
  docker exec -it ksqldb-cli bash -c "ksql http://ksqldb-gde:8088 < /tmp/cmd.sql"
  Pause
}


Pause()
{
  echo;echo "Paused"
  #read Pause
  sleep 30
  clear
  echo;echo
}



sleep 180
clear
echo "Check status - is everything up?"
docker-compose up --detach
docker-compose ps
Pause

echo "Initializing and restarting"
sleep 2
echo :"Truncate, drop, create table in postgres:"

docker         cp config/countries.sql Postgres:/var/tmp/countries.sql
docker-compose -f docker-compose.yml exec postgres bash -c "psql -U postgres postgres -f /var/tmp/countries.sql"
#docker-compose -f docker-compose.yml exec postgres bash -c "psql -U postgres postgres -c \" SELECT * FROM Countries;\" "

docker-compose -f docker-compose.yml exec postgres bash -c "psql -U postgres postgres -c \" CREATE TABLE IF NOT EXISTS GDELT_EVENT ( EVENTID BIGINT NOT NULL, EVENT_DATE VARCHAR(500) , MONTHYEAR VARCHAR(500) , YEAR VARCHAR(500) , FRACTIONDATE VARCHAR(500) , ACTOR1CODE VARCHAR(500), ACTOR1NAME VARCHAR(500), ACTOR1COUNTRYCODE VARCHAR(500), ACTOR1KNOWNGROUPCODE VARCHAR(500), ACTOR1ETHNICCODE VARCHAR(500), ACTOR1RELIGION1CODE VARCHAR(500), ACTOR1RELIGION2CODE VARCHAR(500), ACTOR1TYPE1CODE VARCHAR(500), ACTOR1TYPE2CODE VARCHAR(500), ACTOR1TYPE3CODE VARCHAR(500), ACTOR2CODE VARCHAR(500), ACTOR2NAME VARCHAR(500), ACTOR2COUNTRYCODE VARCHAR(500), ACTOR2KNOWNGROUPCODE VARCHAR(500), ACTOR2ETHNICCODE VARCHAR(500), ACTOR2RELIGION1CODE VARCHAR(500), ACTOR2RELIGION2CODE VARCHAR(500), ACTOR2TYPE1CODE VARCHAR(500), ACTOR2TYPE2CODE VARCHAR(500), ACTOR2TYPE3CODE VARCHAR(500), ISROOTEVENT VARCHAR(500) , EVENTCODE VARCHAR(500), EVENTBASECODE VARCHAR(500), EVENTROOTCODE VARCHAR(500), QUADCLASS VARCHAR(500) , GOLDSTEINSCALE VARCHAR(500) , NUMMENTIONS VARCHAR(500) , NUMSOURCES VARCHAR(500) , NUMARTICLES INTEGER , AVGTONE FLOAT, ACTOR1GEO_TYPE VARCHAR(500) , ACTOR1GEO_FULLNAME VARCHAR(500), ACTOR1GEO_COUNTRYCODE VARCHAR(500), ACTOR1GEO_ADM1CODE VARCHAR(500),ACTOR1GEO_ADM2CODE VARCHAR(500), ACTOR1GEO_LAT VARCHAR(500) , ACTOR1GEO_LONG VARCHAR(500) , ACTOR1GEO_FEATUREID VARCHAR(500), ACTOR2GEO_TYPE VARCHAR(10), ACTOR2GEO_FULLNAME VARCHAR(500), ACTOR2GEO_COUNTRYCODE VARCHAR(500), ACTOR2GEO_ADM1CODE VARCHAR(500),ACTOR2GEO_ADM2CODE VARCHAR(500), ACTOR2GEO_LAT VARCHAR(500) , ACTOR2GEO_LONG VARCHAR(500) , ACTOR2GEO_FEATUREID VARCHAR(500), ACTIONGEO_TYPE VARCHAR(500), ACTIONGEO_FULLNAME VARCHAR(500), ACTIONGEO_COUNTRYCODE VARCHAR(500), ACTIONGEO_ADM1CODE VARCHAR(500),ACTIONGEO_ADM2CODE VARCHAR(500), ACTIONGEO_LAT VARCHAR(500), ACTIONGEO_LONG VARCHAR(500), ACTIONGEO_FEATUREID VARCHAR(500), DATEADDED VARCHAR(200), SOURCEURL VARCHAR(5000));\" "
Pause

FILE=20190712181500.export.csv
echo "Loading  data/gdelt/${FILE} into Postgres ..."
docker cp data/gdelt/${FILE} Postgres:/var/tmp
docker-compose -f docker-compose.yml exec postgres bash -c "psql -U postgres postgres -c \"delete from GDELT_EVENT; \" "
docker-compose -f docker-compose.yml exec postgres bash -c "psql -U postgres postgres -c \"\\copy GDELT_EVENT FROM '/var/tmp/${FILE}' WITH delimiter E'\t' null as ';' \" "
docker-compose -f docker-compose.yml exec postgres bash -c "psql -U postgres postgres -c \"SELECT count(*) from GDELT_EVENT;\" "
Pause

kafka-topics --bootstrap-server kafka1:9092 --topic GDE_000_gdelt_event --create --replication-factor 1
kafka-topics --bootstrap-server kafka1:9092 --topic GDE_000_countries   --create --replication-factor 1
cat <<EOF >/tmp/cmd.sql
CREATE SOURCE CONNECTOR source_jdbc_gdelt_event WITH ( 'connector.class'='io.confluent.connect.jdbc.JdbcSourceConnector', 'connection.url'='jdbc:postgresql://postgres:5432/postgres?user=postgres&password=postgres', 'mode'='incrementing','incrementing.column.name'='eventid', 'topic.prefix'='GDE_000_','table.whitelist'='gdelt_event','numeric.mapping'='best_fit','schema.pattern'='public');
CREATE SOURCE CONNECTOR source_jdbc_countries WITH ( 'connector.class'='io.confluent.connect.jdbc.JdbcSourceConnector', 'connection.url'='jdbc:postgresql://postgres:5432/postgres?user=postgres&password=postgres', 'mode'='incrementing','incrementing.column.name'='id', 'topic.prefix'='GDE_000_','table.whitelist'='countries','numeric.mapping'='best_fit','schema.pattern'='public','topic.creation.default.replication.factor'='1');
EOF
runSQL

cat <<EOF >/tmp/cmd.sql
SHOW CONNECTORS;
EOF
runSQL

cat <<EOF >/tmp/cmd.sql
CREATE STREAM GDE_A010_STR WITH (kafka_topic='GDE_000_gdelt_event', value_format='avro',partitions=1,replicas=1);
CREATE STREAM GDE_C010_STR WITH (kafka_topic='GDE_000_countries', value_format='avro',partitions=1,replicas=1);
EOF
runSQL


#set a key on ACTOR1COUNTRYCODE
cat <<EOF >/tmp/cmd.sql
CREATE STREAM GDE_A020_STR WITH (kafka_topic='GDE_A020_STR', value_format='avro',partitions=1,replicas=1) AS SELECT * FROM GDE_A010_STR PARTITION BY ACTOR1COUNTRYCODE;
EOF
runSQL

#group by 0 is a sneaky little hack
cat <<EOF >/tmp/cmd.sql
CREATE TABLE GDE_B010_TAB WITH (kafka_topic='GDE_B010_TAB',value_format='KAFKA',partitions=1,replicas=1) AS SELECT replace(replace(replace(' {"schema":{"type":"struct","fields":[{"type":"map","keys":{"type":"string","optional":false},"values":{"type":"string","optional":false},"optional":false,"field":"tags"},{"type":"string","optional":false,"field":"time"},{"type":"double","optional":true,"field":"value"}],"optional":false,"version":1},"payload":{"tags":{"id":"REPLACEME_ID"},"time":"REPLACEME_TS","value":REPLACEME_VALUE}}' ,'REPLACEME_ID'   ,'GDE_B010_ROWCOUNT'     ) ,'REPLACEME_TS'   ,cast(max(rowtime) as STRING) ) ,'REPLACEME_VALUE',cast(count(*) as STRING)) as influx_json_row FROM GDE_A020_STR GROUP BY 0;
EOF
runSQL


cat <<EOF >/tmp/cmd.sql
CREATE STREAM GDE_D010_STR WITH (kafka_topic='GDE_D010_STR', value_format='avro',partitions=1,replicas=1) AS SELECT EVENTID as EVENTID, MONTHYEAR , YEAR , FRACTIONDATE , ACTOR1CODE , ACTOR1NAME , ACTOR1COUNTRYCODE , ACTOR1KNOWNGROUPCODE , ACTOR1ETHNICCODE , ACTOR1RELIGION1CODE , ACTOR1RELIGION2CODE , ACTOR1TYPE1CODE , ACTOR1TYPE2CODE , ACTOR1TYPE3CODE , ACTOR2CODE , ACTOR2NAME , ACTOR2COUNTRYCODE , ACTOR2KNOWNGROUPCODE , ACTOR2ETHNICCODE , ACTOR2RELIGION1CODE , ACTOR2RELIGION2CODE , ACTOR2TYPE1CODE , ACTOR2TYPE2CODE , ACTOR2TYPE3CODE , ISROOTEVENT , EVENTCODE , EVENTBASECODE , EVENTROOTCODE , QUADCLASS , GOLDSTEINSCALE , NUMMENTIONS , NUMSOURCES , NUMARTICLES , AVGTONE , ACTOR1GEO_TYPE , ACTOR1GEO_FULLNAME , ACTOR1GEO_COUNTRYCODE , ACTOR1GEO_ADM1CODE , ACTOR1GEO_ADM2CODE , ACTOR1GEO_LAT , ACTOR1GEO_LONG , ACTOR1GEO_FEATUREID , ACTOR2GEO_TYPE , ACTOR2GEO_FULLNAME , ACTOR2GEO_COUNTRYCODE , ACTOR2GEO_ADM1CODE , ACTOR2GEO_ADM2CODE , ACTOR2GEO_LAT , ACTOR2GEO_LONG , ACTOR2GEO_FEATUREID , ACTIONGEO_TYPE , ACTIONGEO_FULLNAME , ACTIONGEO_COUNTRYCODE , ACTIONGEO_ADM1CODE , ACTIONGEO_ADM2CODE , ACTIONGEO_LAT , ACTIONGEO_LONG , ACTIONGEO_FEATUREID , DATEADDED , SOURCEURL, NAME as ACTOR1_COUNTRYNAME FROM GDE_A020_STR JOIN GDE_C010_STR WITHIN 60 MINUTES ON (ACTOR1COUNTRYCODE=ISO3);
EOF
runSQL

cat <<EOF >/tmp/cmd.sql
CREATE TABLE GDE_D020_TAB WITH (kafka_topic='GDE_D020_TAB', value_format='avro',partitions=1,replicas=1) AS SELECT ACTOR1_COUNTRYNAME , cast(count(*) as bigint) as C_COUNT , SUM(AVGTONE) / COUNT(*) as C_AVGTONE,max(AVGTONE) as C_MAXTONE, min(AVGTONE) as C_MINTONE,cast(cast(MAX(cast(EVENTID as bigint)) as STRING) as BIGINT) as LAST_EVENTID FROM GDE_D010_STR GROUP BY ACTOR1_COUNTRYNAME;
EOF
runSQL

#intermediate object - create a stream on the table. Note that this one uses the topic from the prior stmt.
#note that it specific the schema instead of SELECT*From table (which would return Invalid result type. Your SELECT query produces a TABLE. Please use CREATE TABLE AS SELECT statement instead.
cat <<EOF >/tmp/cmd.sql
CREATE STREAM GDE_D030_STR (ACTOR1_COUNTRYNAME string,C_COUNT bigint,C_AVGTONE double,C_MAXTONE double,C_MINTONE double,LAST_EVENTID bigint) WITH (kafka_topic='GDE_D020_TAB', value_format='avro',partitions=1,replicas=1);
EOF
runSQL


#create an InfluxDB Sink Stream of topic data to display country totals 

cat <<EOF >/tmp/cmd.sql
CREATE STREAM GDE_D040_STR WITH (kafka_topic='GDE_D040_STR', value_format='KAFKA',partitions=1,replicas=1) AS SELECT replace(replace(replace(replace(' {"schema":{"type":"struct","fields":[{"type":"map","keys":{"type":"string","optional":false},"values":{"type":"string","optional":false},"optional":false,"field":"tags"},{"type":"string","optional":false,"field":"time"},{"type":"double","optional":true,"field":"value"},{"type":"double","optional":true,"field":"avgtone"}],"optional":false,"version":1},"payload":{"tags":{"id":"REPLACEME_ID"},"time":"REPLACEME_TS","value":REPLACEME_VALUE,"avgtone":REPLACEME_AVGTONE}}','REPLACEME_ID',ACTOR1_COUNTRYNAME),'REPLACEME_TS',cast(rowtime as STRING)),'REPLACEME_VALUE',cast(C_COUNT as STRING)),'REPLACEME_AVGTONE',cast(C_AVGTONE as STRING)) as influx_json_row FROM GDE_D030_STR WHERE ACTOR1_COUNTRYNAME>'';

EOF
runSQL

cat <<EOF >/tmp/cmd.sql
CREATE SINK CONNECTOR sink_inf_D040 WITH ( 'connector.class'='io.confluent.influxdb.InfluxDBSinkConnector','tasks.max'='1','topics'='GDE_D040_STR','influxdb.url'='http://influxdb:8086', 'influxdb.db'='gdelt','influx.db.username'='root','influxdb.password'='root','measurement.name.format'='GDE_D040_STR','value.converter'='org.apache.kafka.connect.json.JsonConverter');

CREATE SINK CONNECTOR sink_inf_B010 WITH ( 'connector.class'='io.confluent.influxdb.InfluxDBSinkConnector','tasks.max'='1','topics'='GDE_B010_TAB','influxdb.url'='http://influxdb:8086', 'influxdb.db'='gdelt','influx.db.username'='root','influxdb.password'='root','measurement.name.format'='GDE_B010_TAB','value.converter'='org.apache.kafka.connect.json.JsonConverter');
EOF
runSQL

clear
FILE=20191205000000.export.csv
echo "Loading  data/gdelt/${FILE} into Postgres ..."
docker cp data/gdelt/${FILE} Postgres:/var/tmp
docker-compose -f docker-compose.yml exec postgres bash -c "psql -U postgres postgres -c \"\\copy GDELT_EVENT FROM '/var/tmp/${FILE}' WITH delimiter E'\t' null as ';' \" "
docker-compose -f docker-compose.yml exec postgres bash -c "psql -U postgres postgres -c \"SELECT count(*) from GDELT_EVENT;\" "


echo;echo;echo "Finished!"
echo "Run getNews to load up yesterdays news"


